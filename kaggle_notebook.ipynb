{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7950b7ab",
   "metadata": {},
   "source": [
    "#  Stock Price Extractor - Kaggle Notebook\n",
    "\n",
    "Extract stock price information from audio files using LLM-based extraction.\n",
    "\n",
    "**Features:**\n",
    "-  Whisper: Audio transcription to text\n",
    "-  Mistral-3B LLM: Financial data extraction (smallest, fastest model)\n",
    "-  GPU T4: Fast processing (<5 seconds per audio file)\n",
    "-  JSON output: Structured results ready to download\n",
    "\n",
    "**Prerequisites:**\n",
    "-  GPU T4 enabled (Settings ‚Üí Accelerator ‚Üí GPU T4)\n",
    "-  HuggingFace account (Mistral models are open, no license acceptance needed)\n",
    "-  HuggingFace API token created\n",
    "\n",
    "**Performance:**\n",
    "-  Mistral-3B: Smallest model (~6GB memory, faster than 7B)\n",
    "-  Target: <5 seconds per file\n",
    "-  Transcription: <2s, Extraction: <2s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed308235",
   "metadata": {},
   "source": [
    "## Step 1Ô∏è‚É£: Verify GPU and Find Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69427a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\" KAGGLE STOCK PRICE EXTRACTOR\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# 1. Verify GPU\n",
    "print(\"\\n‚úì GPU Check:\")\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f\"  GPU Available: {gpu_available}\")\n",
    "if gpu_available:\n",
    "    print(f\"  Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"  Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "else:\n",
    "    print(\"    GPU not enabled - enable in Settings ‚Üí Accelerator ‚Üí GPU T4\")\n",
    "\n",
    "# 2. Find uploaded files\n",
    "print(\"\\n‚úì Finding uploaded files:\")\n",
    "audio_files = sorted(list(Path('/kaggle/input').glob('**/*.wav')))\n",
    "prompt_files = sorted(list(Path('/kaggle/input').glob('**/*.txt')))\n",
    "\n",
    "print(f\"  Audio files found: {len(audio_files)}\")\n",
    "for f in audio_files[:3]:\n",
    "    size_mb = f.stat().st_size / (1024 * 1024)\n",
    "    print(f\"    ‚Ä¢ {f.name} ({size_mb:.1f} MB)\")\n",
    "\n",
    "print(f\"  Prompt files found: {len(prompt_files)}\")\n",
    "for f in prompt_files[:2]:\n",
    "    print(f\"    ‚Ä¢ {f.name}\")\n",
    "\n",
    "# 3. Set paths\n",
    "if audio_files and prompt_files:\n",
    "    audio_path = str(audio_files[0])\n",
    "    prompt_path = str(prompt_files[0])\n",
    "    print(f\"\\n Ready to process!\")\n",
    "    print(f\"  Audio: {Path(audio_path).name}\")\n",
    "    print(f\"  Prompt: {Path(prompt_path).name}\")\n",
    "else:\n",
    "    print(f\"\\n Missing files - cannot proceed\")\n",
    "    audio_path = None\n",
    "    prompt_path = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3f9794",
   "metadata": {},
   "source": [
    "## Step 2Ô∏è‚É£: Configure HuggingFace Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9090566b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "#  REPLACE WITH YOUR TOKEN\n",
    "HF_TOKEN = \"hf_YOUR_TOKEN_HERE\"\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" HUGGINGFACE AUTHENTICATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if HF_TOKEN == \"hf_YOUR_TOKEN_HERE\":\n",
    "    print(\"\\n  Token not configured!\")\n",
    "    print(\"\\n Setup Instructions:\")\n",
    "    print(\"  1. Go to: https://huggingface.co/mistralai/Mistral-3B-Instruct-v0.1\")\n",
    "    print(\"  2. Mistral models are open - no license acceptance needed!\")\n",
    "    print(\"  3. Get your token: https://huggingface.co/settings/tokens\")\n",
    "    print(\"  4. Replace 'hf_YOUR_TOKEN_HERE' above with your actual token\")\n",
    "    print(\"  5. Re-run this cell\\n\")\n",
    "else:\n",
    "    try:\n",
    "        login(token=HF_TOKEN)\n",
    "        print(\"\\n‚úÖ Successfully logged into HuggingFace\")\n",
    "        print(\"   Using Mistral-3B-Instruct-v0.1 (smallest, fastest model)\")\n",
    "        print(\"   üí° 3B model: ~6GB memory, faster than 7B!\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Authentication failed: {e}\")\n",
    "        print(\"  Check that your token is correct\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86917dd6",
   "metadata": {},
   "source": [
    "## Step 3Ô∏è‚É£: Load Models (Transcribe & Extract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35ac263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" LOADING MODELS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Auto-detect app/models/transcribe.py anywhere under /kaggle/input\n",
    "root_dir = Path(\"/kaggle/input\")\n",
    "app_source = None\n",
    "\n",
    "print(\"\\n Searching for app/models/transcribe.py under /kaggle/input...\")\n",
    "for transcribe_path in root_dir.rglob(\"transcribe.py\"):\n",
    "    # We want .../app/models/transcribe.py specifically\n",
    "    if (\n",
    "        transcribe_path.name == \"transcribe.py\"\n",
    "        and transcribe_path.parent.name == \"models\"\n",
    "        and transcribe_path.parent.parent.name == \"app\"\n",
    "    ):\n",
    "        app_source = transcribe_path.parent.parent\n",
    "        print(f\" Found app folder at: {app_source}\")\n",
    "        break\n",
    "\n",
    "if not app_source:\n",
    "    print(\" app folder not found!\")\n",
    "    raise FileNotFoundError(\"app/models/transcribe.py not found under /kaggle/input. Check your dataset structure.\")\n",
    "\n",
    "# Add parent to path so 'from app.models...' works\n",
    "sys.path.insert(0, str(app_source.parent))\n",
    "\n",
    "# Copy to working directory for persistence\n",
    "app_working = Path('/kaggle/working') / 'app'\n",
    "if not app_working.exists():\n",
    "    try:\n",
    "        shutil.copytree(app_source, app_working)\n",
    "        sys.path.insert(0, '/kaggle/working')\n",
    "    except Exception as e:\n",
    "        print(f\"  Copy failed: {e}\")\n",
    "\n",
    "# Import models\n",
    "print(\"\\n Importing functions...\")\n",
    "try:\n",
    "    from app.models.transcribe import transcribe\n",
    "    from app.models.llm_extract import extract_with_long_prompt\n",
    "    print(\" transcribe() - ready\")\n",
    "    print(\" extract_with_long_prompt() - ready\")\n",
    "except ImportError as e:\n",
    "    print(f\" Import failed: {e}\")\n",
    "    print(f\"   sys.path[0] = {sys.path[0]}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f0d01d",
   "metadata": {},
   "source": [
    "## Step 4Ô∏è‚É£: Transcribe Audio to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af2dfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" TRANSCRIPTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if not audio_path:\n",
    "    print(\" No audio file available - skipping transcription\")\n",
    "    transcript = None\n",
    "else:\n",
    "    print(f\"\\n File: {Path(audio_path).name}\")\n",
    "    \n",
    "    try:\n",
    "        start = time.time()\n",
    "        print(\" Processing... (5-10 seconds)\")\n",
    "        \n",
    "        # Fix for precision error: set torch to use float32 consistently\n",
    "        torch.set_default_dtype(torch.float32)\n",
    "        \n",
    "        # Force model to use float32\n",
    "        os.environ['TORCH_DTYPE'] = 'float32'\n",
    "        \n",
    "        # Transcribe audio\n",
    "        result = transcribe(audio_path)\n",
    "        \n",
    "        # Handle different return formats\n",
    "        if isinstance(result, dict):\n",
    "            transcript = result.get('result', result)\n",
    "        else:\n",
    "            transcript = result\n",
    "        \n",
    "        # Ensure transcript is a string\n",
    "        if isinstance(transcript, dict):\n",
    "            transcript = str(transcript)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\" Done in {elapsed:.1f}s\\n\")\n",
    "        \n",
    "        # Show transcript\n",
    "        if transcript and len(str(transcript)) > 0:\n",
    "            preview = str(transcript)[:300] + (\"...\" if len(str(transcript)) > 300 else \"\")\n",
    "            print(f\" Transcript ({len(str(transcript))} chars):\\n{preview}\\n\")\n",
    "        else:\n",
    "            print(\"  Empty transcript - audio may be silent or corrupted\")\n",
    "            \n",
    "    except RuntimeError as e:\n",
    "        if \"float\" in str(e) or \"Half\" in str(e) or \"dtype\" in str(e):\n",
    "            print(f\"  GPU precision issue detected\")\n",
    "            print(f\"   Attempting direct transcription...\\n\")\n",
    "            \n",
    "            try:\n",
    "                # Direct approach: load model with explicit dtype\n",
    "                import librosa\n",
    "                from transformers import pipeline\n",
    "                \n",
    "                # Load audio\n",
    "                audio_data, sr = librosa.load(audio_path, sr=16000)\n",
    "                \n",
    "                # Create pipeline with explicit float32\n",
    "                pipe = pipeline(\n",
    "                    \"automatic-speech-recognition\",\n",
    "                    model=\"openai/whisper-small\",\n",
    "                    device=0 if torch.cuda.is_available() else -1,\n",
    "                    torch_dtype=torch.float32\n",
    "                )\n",
    "                \n",
    "                result = pipe(audio_data)\n",
    "                transcript = result.get('text', str(result))\n",
    "                \n",
    "                elapsed = time.time() - start\n",
    "                print(f\" Transcription complete in {elapsed:.1f}s\\n\")\n",
    "                \n",
    "                preview = str(transcript)[:300] + (\"...\" if len(str(transcript)) > 300 else \"\")\n",
    "                print(f\" Transcript ({len(str(transcript))} chars):\\n{preview}\\n\")\n",
    "                \n",
    "            except Exception as direct_e:\n",
    "                print(f\" Direct transcription also failed: {direct_e}\")\n",
    "                print(f\"\\n Audio file may be corrupted or invalid\")\n",
    "                print(f\"   Please check the audio file and try again\")\n",
    "                transcript = None\n",
    "        else:\n",
    "            print(f\" Error: {e}\")\n",
    "            transcript = None\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" Error: {e}\")\n",
    "        print(f\"   Type: {type(e).__name__}\")\n",
    "        transcript = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6501eb3",
   "metadata": {},
   "source": [
    "## Step 5Ô∏è‚É£: Extract Stock Price Data with LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f24ad243",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" LLM EXTRACTION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "result = None\n",
    "\n",
    "if not transcript:\n",
    "    print(\"\\n No transcript available - cannot extract\")\n",
    "elif not prompt_path:\n",
    "    print(\"\\n No prompt file available - cannot extract\")\n",
    "else:\n",
    "    print(f\"\\n Prompt: {Path(prompt_path).name}\")\n",
    "    \n",
    "    try:\n",
    "        start = time.time()\n",
    "        print(\" Extracting... (2-5 seconds with GPU)\")\n",
    "        \n",
    "        # Extract using LLM\n",
    "        result = extract_with_long_prompt(transcript, prompt_file=prompt_path)\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        print(f\" Done in {elapsed:.1f}s\\n\")\n",
    "        \n",
    "        # Display results\n",
    "        if result:\n",
    "            print(\" Extracted Data:\")\n",
    "            print(json.dumps(result, indent=2))\n",
    "        else:\n",
    "            print(\"  No data extracted\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\" Error: {e}\")\n",
    "        print(\"\\n Troubleshooting:\")\n",
    "        print(\"  ‚Ä¢ Check HuggingFace token is valid\")\n",
    "        print(\"  ‚Ä¢ Verify Mistral model can be accessed\")\n",
    "        print(\"  ‚Ä¢ Check internet connection\")\n",
    "        print(\"  ‚Ä¢ Ensure GPU has enough memory (Mistral 7B needs ~14GB)\")\n",
    "\n",
    "# Clear GPU memory\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c3e13c0",
   "metadata": {},
   "source": [
    "## Step 6Ô∏è‚É£: Save and Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99170875",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" SAVING RESULTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "output_dir = Path('/kaggle/working')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "if result:\n",
    "    # Save extraction results\n",
    "    results_file = output_dir / 'stock_price_extraction.json'\n",
    "    with open(results_file, 'w') as f:\n",
    "        json.dump(result, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n Results saved: stock_price_extraction.json\")\n",
    "    print(f\"   Size: {results_file.stat().st_size} bytes\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'audio_file': Path(audio_path).name if audio_path else None,\n",
    "        'prompt_file': Path(prompt_path).name if prompt_path else None,\n",
    "        'gpu_device': torch.cuda.get_device_name(0) if torch.cuda.is_available() else 'CPU',\n",
    "        'extraction_status': 'success'\n",
    "    }\n",
    "    \n",
    "    metadata_file = output_dir / 'metadata.json'\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    \n",
    "    print(f\" Metadata saved: metadata.json\")\n",
    "    \n",
    "    print(f\"\\n Download your files:\")\n",
    "    print(f\"  1. Click 'Files' (right panel)\")\n",
    "    print(f\"  2. Download 'stock_price_extraction.json'\")\n",
    "    print(f\"  3. Download 'metadata.json'\")\n",
    "    \n",
    "else:\n",
    "    print(f\"\\n  No results to save (extraction may have failed)\")\n",
    "    \n",
    "    # Save error info\n",
    "    error_info = {\n",
    "        'audio_found': audio_path is not None,\n",
    "        'prompt_found': prompt_path is not None,\n",
    "        'transcript_generated': transcript is not None,\n",
    "        'gpu_available': torch.cuda.is_available()\n",
    "    }\n",
    "    \n",
    "    error_file = output_dir / 'error_info.json'\n",
    "    with open(error_file, 'w') as f:\n",
    "        json.dump(error_info, f, indent=2)\n",
    "    \n",
    "    print(f\"   Saved debug info to: error_info.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" COMPLETE\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366123b9",
   "metadata": {},
   "source": [
    "## Step 7Ô∏è‚É£: Process Multiple Audio Files (Optional)\n",
    "\n",
    "Process all audio files in the dataset and save individual results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fa40fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\" BATCH PROCESSING\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Find all audio files\n",
    "all_audio_files = sorted(list(Path('/kaggle/input').glob('**/*.wav')))\n",
    "\n",
    "if len(all_audio_files) <= 1:\n",
    "    print(f\"\\n‚úì Only 1 audio file - already processed above\")\n",
    "else:\n",
    "    print(f\"\\n‚úì Found {len(all_audio_files)} audio files\")\n",
    "    print(f\"  Processing all of them...\\n\")\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    for i, audio_file in enumerate(all_audio_files, 1):\n",
    "        filename = audio_file.name\n",
    "        print(f\"[{i}/{len(all_audio_files)}] {filename}...\", end=' ', flush=True)\n",
    "        \n",
    "        try:\n",
    "            # Clear memory\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Transcribe\n",
    "            trans_result = transcribe(str(audio_file))\n",
    "            trans = trans_result.get('result') if isinstance(trans_result, dict) else trans_result\n",
    "            \n",
    "            # Extract\n",
    "            extract_result = extract_with_long_prompt(trans, prompt_file=prompt_path)\n",
    "            \n",
    "            all_results[filename] = {\n",
    "                'status': 'success',\n",
    "                'data': extract_result\n",
    "            }\n",
    "            print(\"\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            all_results[filename] = {\n",
    "                'status': 'error',\n",
    "                'error': str(e)\n",
    "            }\n",
    "            print(f\" ({str(e)[:30]}...)\")\n",
    "    \n",
    "    # Save batch results\n",
    "    batch_file = Path('/kaggle/working') / 'batch_results.json'\n",
    "    with open(batch_file, 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n All results saved to: batch_results.json\")\n",
    "    \n",
    "    # Summary\n",
    "    success_count = sum(1 for v in all_results.values() if v['status'] == 'success')\n",
    "    error_count = len(all_results) - success_count\n",
    "    \n",
    "    print(f\"\\n Summary:\")\n",
    "    print(f\"   Successful: {success_count}/{len(all_results)}\")\n",
    "    print(f\"   Failed: {error_count}/{len(all_results)}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
