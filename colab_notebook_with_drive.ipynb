{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b904ed0f",
   "metadata": {},
   "source": [
    "## Step 1ï¸âƒ£: Mount Google Drive & Setup Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be74877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"âœ… Google Drive mounted!\")\n",
    "\n",
    "# Create model cache directory in Google Drive\n",
    "# This ensures models persist between sessions!\n",
    "model_cache = Path('/content/drive/MyDrive/Stock-Price-Extractor-Cache')\n",
    "model_cache.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"âœ… Model cache directory: {model_cache}\")\n",
    "print(f\"   ğŸ“Š Cache size: {sum(f.stat().st_size for f in model_cache.rglob('*') if f.is_file()) / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1936534",
   "metadata": {},
   "source": [
    "## Step 2ï¸âƒ£: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ccc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch librosa soundfile accelerate huggingface-hub vllm\n",
    "\n",
    "print(\"âœ… Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00301650",
   "metadata": {},
   "source": [
    "## Step 3ï¸âƒ£: Clone Repository & Set Model Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04478388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Clone the project\n",
    "!git clone https://github.com/hasipfaruk/Stock-Price-Extractor.git\n",
    "\n",
    "# Navigate to project\n",
    "os.chdir(\"Stock-Price-Extractor\")\n",
    "print(\"âœ… Project cloned!\")\n",
    "print(f\"ğŸ“ Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Set HuggingFace cache to Google Drive for persistence\n",
    "model_cache = '/content/drive/MyDrive/Stock-Price-Extractor-Cache'\n",
    "os.environ['HF_HOME'] = model_cache\n",
    "os.environ['TRANSFORMERS_CACHE'] = model_cache\n",
    "\n",
    "print(f\"\\nâœ… Model cache set to Google Drive:\")\n",
    "print(f\"   ğŸ“ {model_cache}\")\n",
    "print(f\"\\nğŸ’¡ Models will persist between sessions!\")\n",
    "print(f\"   First run: Download & save to Google Drive\")\n",
    "print(f\"   Next runs: Reuse from Google Drive (instant!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d450ac5c",
   "metadata": {},
   "source": [
    "## Step 4ï¸âƒ£: HuggingFace Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd5cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Set your token (replace with yours)\n",
    "HF_TOKEN = \"hf_YOUR_TOKEN_HERE\"\n",
    "\n",
    "if HF_TOKEN == \"hf_YOUR_TOKEN_HERE\":\n",
    "    print(\"âš ï¸ IMPORTANT: Replace with your actual HuggingFace token!\")\n",
    "    print(\"ğŸ“– Get token from: https://huggingface.co/settings/tokens\")\n",
    "    print(\"1. Go to the link above\")\n",
    "    print(\"2. Create new token (read access is fine)\")\n",
    "    print(\"3. Copy and paste in the line above\")\n",
    "    print(\"\\nâš ï¸ Also, make sure you've accepted Llama 2 license:\")\n",
    "    print(\"   https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\")\n",
    "else:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"âœ… Authenticated with HuggingFace!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29b7752",
   "metadata": {},
   "source": [
    "## Step 5ï¸âƒ£: Verify GPU & Check Cached Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b606c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GPU STATUS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f\"âœ… GPU Available: {gpu_available}\")\n",
    "\n",
    "if gpu_available:\n",
    "    print(f\"ğŸ“Š GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"ğŸ’¾ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(\"\\nâœ¨ GPU enabled - processing will be FAST!\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ GPU not available - will use CPU (slower)\")\n",
    "    print(\"ğŸ’¡ To enable GPU: Runtime â†’ Change runtime type â†’ GPU\")\n",
    "\n",
    "# Check cached models in Google Drive\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CACHED MODELS (Google Drive)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cache_dir = Path('/content/drive/MyDrive/Stock-Price-Extractor-Cache')\n",
    "if cache_dir.exists():\n",
    "    models = list(cache_dir.glob('models--*/snapshots/*/model*.safetensors'))\n",
    "    if models:\n",
    "        print(f\"\\nâœ… Found {len(models)} cached models:\")\n",
    "        total_size = 0\n",
    "        for model_file in models:\n",
    "            size_mb = model_file.stat().st_size / (1024**2)\n",
    "            total_size += model_file.stat().st_size\n",
    "            model_name = model_file.parent.parent.parent.name.replace('models--', '').replace('--', '/')\n",
    "            print(f\"  ğŸ“¦ {model_name}: {size_mb:.0f} MB\")\n",
    "        print(f\"\\nğŸ“Š Total cached: {total_size / (1024**3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"\\nğŸ†• No cached models yet. Will download on first run.\")\n",
    "        print(\"   First run: ~10-15 minutes (includes model downloads)\")\n",
    "        print(\"   Next runs: Instant (models cached)\")\n",
    "else:\n",
    "    print(\"\\nğŸ†• Cache directory created. Ready for downloads!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63a3ba3",
   "metadata": {},
   "source": [
    "## Step 6ï¸âƒ£: Upload Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ee5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create upload directory\n",
    "os.makedirs('uploaded_audio', exist_ok=True)\n",
    "\n",
    "print(\"ğŸ“ Upload audio files:\")\n",
    "print(\"1. Click 'Choose Files'\")\n",
    "print(\"2. Select multiple audio files (WAV, MP3, FLAC, M4A)\")\n",
    "print(\"3. Wait for upload to complete\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "print(f\"\\nâœ… {len(uploaded)} files uploaded:\")\n",
    "for filename in uploaded.keys():\n",
    "    file_path = f'uploaded_audio/{filename}'\n",
    "    os.rename(filename, file_path)\n",
    "    file_size = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    print(f\"  ğŸ“„ {filename} ({file_size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6837016",
   "metadata": {},
   "source": [
    "## Step 7ï¸âƒ£: Upload Extraction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ee438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Upload your prompt file\n",
    "print(\"ğŸ“ Upload extraction prompt file:\")\n",
    "print(\"Click 'Choose Files' and select your prompt.txt file\\n\")\n",
    "\n",
    "prompt_files = files.upload()\n",
    "\n",
    "if prompt_files:\n",
    "    prompt_filename = list(prompt_files.keys())[0]\n",
    "    os.rename(prompt_filename, 'colab_prompt.txt')\n",
    "    print(f\"âœ… Prompt uploaded: {prompt_filename}\")\n",
    "    \n",
    "    # Show first 200 chars\n",
    "    with open('colab_prompt.txt', 'r') as f:\n",
    "        content = f.read()\n",
    "    print(f\"\\nğŸ“– Prompt preview ({len(content)} chars):\")\n",
    "    print(content[:200] + \"...\\n\" if len(content) > 200 else content)\n",
    "else:\n",
    "    print(\"âš ï¸ No prompt file uploaded. Using default prompt.\")\n",
    "    # Create default prompt\n",
    "    default_prompt = \"\"\"Extract stock price information from the transcript.\n",
    "\n",
    "Return JSON with these fields:\n",
    "- index_name: Stock index name (e.g., \\\"S&P 500\\\")\n",
    "- price: Current price\n",
    "- change: Change in points\n",
    "- change_percent: Percent change\n",
    "\n",
    "Return ONLY valid JSON, no explanation.\"\"\"\n",
    "    \n",
    "    with open('colab_prompt.txt', 'w') as f:\n",
    "        f.write(default_prompt)\n",
    "    print(\"âœ… Default prompt created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c240f8",
   "metadata": {},
   "source": [
    "## Step 8ï¸âƒ£: Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c96c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project to path\n",
    "project_path = Path.cwd()\n",
    "sys.path.insert(0, str(project_path))\n",
    "\n",
    "# Import functions\n",
    "from app.models.transcribe import transcribe\n",
    "from app.models.llm_extract import extract_with_long_prompt\n",
    "\n",
    "print(\"âœ… Functions imported successfully!\")\n",
    "print(f\"  ğŸ“ transcribe() - ready\")\n",
    "print(f\"  ğŸ“ extract_with_long_prompt() - ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e724c481",
   "metadata": {},
   "source": [
    "## Step 9ï¸âƒ£: Batch Processing (with Timing & Drive Cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298553be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Find all audio files\n",
    "audio_files = sorted(list(Path('uploaded_audio').glob('*')))\n",
    "audio_files = [f for f in audio_files if f.suffix.lower() in ['.wav', '.mp3', '.flac', '.m4a', '.ogg']]\n",
    "\n",
    "print(f\"ğŸ“ Found {len(audio_files)} audio files\\n\")\n",
    "\n",
    "if len(audio_files) == 0:\n",
    "    print(\"âŒ No audio files to process\")\n",
    "else:\n",
    "    all_results = {}\n",
    "    batch_start_time = time.time()\n",
    "    \n",
    "    print(\"ğŸ”„ PROCESSING AUDIO FILES (Models cached in Google Drive)\\n\")\n",
    "    \n",
    "    for i, audio_file in enumerate(audio_files, 1):\n",
    "        filename = audio_file.name\n",
    "        file_start_time = time.time()\n",
    "        print(f\"[{i}/{len(audio_files)}] Processing {filename}...\", end=' ', flush=True)\n",
    "        \n",
    "        try:\n",
    "            # Clear memory\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Transcribe\n",
    "            trans_start = time.time()\n",
    "            result = transcribe(str(audio_file))\n",
    "            trans_duration = time.time() - trans_start\n",
    "            \n",
    "            transcript = result.get('result') if isinstance(result, dict) else result\n",
    "            trans_time = result.get('time', trans_duration) if isinstance(result, dict) else trans_duration\n",
    "            \n",
    "            # Extract\n",
    "            extract_start = time.time()\n",
    "            extraction = extract_with_long_prompt(transcript, prompt_file='colab_prompt.txt')\n",
    "            extract_duration = time.time() - extract_start\n",
    "            \n",
    "            file_total = time.time() - file_start_time\n",
    "            \n",
    "            all_results[filename] = {\n",
    "                \"status\": \"success\",\n",
    "                \"data\": extraction,\n",
    "                \"timing\": {\n",
    "                    \"transcription_s\": round(trans_time, 3),\n",
    "                    \"extraction_s\": round(extract_duration, 3),\n",
    "                    \"total_s\": round(file_total, 3)\n",
    "                }\n",
    "            }\n",
    "            print(f\"âœ… ({file_total:.2f}s)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            file_total = time.time() - file_start_time\n",
    "            all_results[filename] = {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)[:100],\n",
    "                \"timing\": {\n",
    "                    \"total_s\": round(file_total, 3)\n",
    "                }\n",
    "            }\n",
    "            print(f\"âŒ ({file_total:.2f}s)\")\n",
    "    \n",
    "    # Save results\n",
    "    with open('batch_results.json', 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    batch_total_time = time.time() - batch_start_time\n",
    "    print(f\"\\nâœ… All results saved to: batch_results.json\")\n",
    "    \n",
    "    # Summary with timing\n",
    "    success = sum(1 for r in all_results.values() if r[\"status\"] == \"success\")\n",
    "    failed = len(all_results) - success\n",
    "    avg_time = sum(r.get(\"timing\", {}).get(\"total_s\", 0) for r in all_results.values() if r[\"status\"] == \"success\") / max(success, 1)\n",
    "    \n",
    "    print(f\"\\nğŸ“Š Summary:\")\n",
    "    print(f\"  âœ… Successful: {success}/{len(all_results)}\")\n",
    "    print(f\"  âŒ Failed: {failed}/{len(all_results)}\")\n",
    "    print(f\"  â±ï¸ Batch Total: {batch_total_time:.2f}s\")\n",
    "    print(f\"  â±ï¸ Average per file: {avg_time:.2f}s\")\n",
    "    \n",
    "    print(f\"\\nğŸ’¾ Google Drive Cache Info:\")\n",
    "    cache_dir = Path('/content/drive/MyDrive/Stock-Price-Extractor-Cache')\n",
    "    if cache_dir.exists():\n",
    "        total_cache = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())\n",
    "        print(f\"  ğŸ“ Cache Directory: {cache_dir}\")\n",
    "        print(f\"  ğŸ“Š Total Cached: {total_cache / (1024**3):.2f} GB\")\n",
    "        print(f\"  âœ… Persisted across sessions!\")\n",
    "    \n",
    "    # Show sample results with timing\n",
    "    if success > 0:\n",
    "        print(f\"\\nğŸ“ˆ Sample Results:\")\n",
    "        for filename, result in list(all_results.items())[:3]:\n",
    "            if result[\"status\"] == \"success\":\n",
    "                print(f\"\\n  {filename}:\")\n",
    "                data = result[\"data\"]\n",
    "                timing = result.get(\"timing\", {})\n",
    "                print(f\"    Index: {data.get('index_name')}\")\n",
    "                print(f\"    Price: {data.get('price')}\")\n",
    "                print(f\"    Change: {data.get('change')} ({data.get('change_percent')})\")\n",
    "                print(f\"    ğŸ¤ Transcription: {timing.get('transcription_s', 0):.3f}s\")\n",
    "                print(f\"    ğŸ¤– Extraction: {timing.get('extraction_s', 0):.3f}s\")\n",
    "                print(f\"    â±ï¸ Total: {timing.get('total_s', 0):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e4211",
   "metadata": {},
   "source": [
    "## Step ğŸ”Ÿ: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"ğŸ“¥ Download your results:\\n\")\n",
    "\n",
    "# Download batch results if exists\n",
    "if Path('batch_results.json').exists():\n",
    "    print(\"1. Downloading batch_results.json (with timing data)...\")\n",
    "    files.download('batch_results.json')\n",
    "    print(\"   âœ… Downloaded!\")\n",
    "\n",
    "print(\"\\nâœ… All files ready for download!\")\n",
    "print(\"\\nCheck the 'Files' tab (left panel) to download any files.\")\n",
    "print(\"\\nğŸ“Š Results include:\")\n",
    "print(\"  - Index name, price, change, change%\")\n",
    "print(\"  - ğŸ¤ Transcription time (seconds)\")\n",
    "print(\"  - ğŸ¤– Extraction time (seconds)\")\n",
    "print(\"  - â±ï¸ Total time per file (seconds)\")\n",
    "\n",
    "print(\"\\nğŸ’¾ Google Drive Model Cache:\")\n",
    "print(\"  - Models saved in: /MyDrive/Stock-Price-Extractor-Cache\")\n",
    "print(\"  - Reused in next Colab sessions (no re-download!)\")\n",
    "print(\"  - Saves 10-15 minutes on next run! ğŸš€\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e700785",
   "metadata": {},
   "source": [
    "## ğŸ‰ Complete!\n",
    "\n",
    "Your stock price extraction is done! ğŸš€\n",
    "\n",
    "**âœ¨ What's Special:**\n",
    "1. âœ… Stock prices extracted from audio\n",
    "2. ğŸ¤ Transcription time for each file\n",
    "3. ğŸ¤– LLM extraction time for each file\n",
    "4. â±ï¸ Total processing time per file\n",
    "5. ğŸ’¾ **Models saved to Google Drive (persistent!)**\n",
    "\n",
    "**Next Session:**\n",
    "- Run this notebook again with new audio files\n",
    "- Models are already in Google Drive\n",
    "- **Skip model downloads (saves 10-15 minutes!)**\n",
    "- Just upload new audio â†’ Process â†’ Download results\n",
    "\n",
    "**Next steps:**\n",
    "1. Download `batch_results.json` from the Files panel\n",
    "2. Analyze timing data to optimize performance\n",
    "3. Import results into your spreadsheet or database\n",
    "\n",
    "**For more info:**\n",
    "- See `README.md` for general documentation\n",
    "- See `TIMING_FEATURES.md` for timing guide\n",
    "- See `MODEL_CACHING.md` for model persistence guide\n",
    "- See `USAGE.md` for usage examples"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
