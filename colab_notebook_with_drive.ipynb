{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b904ed0f",
   "metadata": {},
   "source": [
    "## Step 1Ô∏è‚É£: Mount Google Drive & Setup Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be74877",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "print(\"‚úÖ Google Drive mounted!\")\n",
    "\n",
    "# Create model cache directory in Google Drive\n",
    "# This ensures models persist between sessions!\n",
    "model_cache = Path('/content/drive/MyDrive/Stock-Price-Extractor-Cache')\n",
    "model_cache.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "print(f\"‚úÖ Model cache directory: {model_cache}\")\n",
    "print(f\"   üìä Cache size: {sum(f.stat().st_size for f in model_cache.rglob('*') if f.is_file()) / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1936534",
   "metadata": {},
   "source": [
    "## Step 2Ô∏è‚É£: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1ccc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q transformers torch librosa soundfile accelerate huggingface-hub vllm\n",
    "\n",
    "print(\"‚úÖ Dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00301650",
   "metadata": {},
   "source": [
    "## Step 3Ô∏è‚É£: Clone Repository & Set Model Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04478388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Clone the project\n",
    "!git clone https://github.com/hasipfaruk/Stock-Price-Extractor.git\n",
    "\n",
    "# Navigate to project\n",
    "os.chdir(\"Stock-Price-Extractor\")\n",
    "print(\"‚úÖ Project cloned!\")\n",
    "print(f\"üìÅ Current directory: {os.getcwd()}\")\n",
    "\n",
    "# Set HuggingFace cache to Google Drive for persistence\n",
    "model_cache = '/content/drive/MyDrive/Stock-Price-Extractor-Cache'\n",
    "os.environ['HF_HOME'] = model_cache\n",
    "os.environ['TRANSFORMERS_CACHE'] = model_cache\n",
    "\n",
    "print(f\"\\n‚úÖ Model cache set to Google Drive:\")\n",
    "print(f\"   üìÅ {model_cache}\")\n",
    "print(f\"\\nüí° Models will persist between sessions!\")\n",
    "print(f\"   First run: Download & save to Google Drive\")\n",
    "print(f\"   Next runs: Reuse from Google Drive (instant!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d450ac5c",
   "metadata": {},
   "source": [
    "## Step 4Ô∏è‚É£: HuggingFace Authentication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4cd5cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Set your token (replace with yours)\n",
    "HF_TOKEN = \"hf_YOUR_TOKEN_HERE\"\n",
    "\n",
    "if HF_TOKEN == \"hf_YOUR_TOKEN_HERE\":\n",
    "    print(\"‚ö†Ô∏è IMPORTANT: Replace with your actual HuggingFace token!\")\n",
    "    print(\"üìñ Get token from: https://huggingface.co/settings/tokens\")\n",
    "    print(\"1. Go to the link above\")\n",
    "    print(\"2. Create new token (read access is fine)\")\n",
    "    print(\"3. Copy and paste in the line above\")\n",
    "    print(\"\\n‚úÖ Mistral models are open - no license acceptance needed!\")\n",
    "    print(\"   Using: mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "else:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úÖ Authenticated with HuggingFace!\")\n",
    "    print(\"   Using Mistral-7B-Instruct-v0.2 (optimized for <6s processing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a29b7752",
   "metadata": {},
   "source": [
    "## Step 5Ô∏è‚É£: Verify GPU & Check Cached Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b606c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"GPU STATUS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "gpu_available = torch.cuda.is_available()\n",
    "print(f\"‚úÖ GPU Available: {gpu_available}\")\n",
    "\n",
    "if gpu_available:\n",
    "    print(f\"üìä GPU Device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    print(\"\\n‚ú® GPU enabled - processing optimized for <6s per file!\")\n",
    "    print(\"   ‚ö° Transcription: <2s (Distil-Whisper)\")\n",
    "    print(\"   ‚ö° LLM Extraction: <3s (Mistral-7B)\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è GPU not available - will use CPU (slower)\")\n",
    "    print(\"üí° To enable GPU: Runtime ‚Üí Change runtime type ‚Üí GPU\")\n",
    "    print(\"   ‚ö†Ô∏è CPU mode will be slower than <6s target\")\n",
    "\n",
    "# Check cached models in Google Drive\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CACHED MODELS (Google Drive)\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cache_dir = Path('/content/drive/MyDrive/Stock-Price-Extractor-Cache')\n",
    "if cache_dir.exists():\n",
    "    models = list(cache_dir.glob('models--*/snapshots/*/model*.safetensors'))\n",
    "    if models:\n",
    "        print(f\"\\n‚úÖ Found {len(models)} cached models:\")\n",
    "        total_size = 0\n",
    "        for model_file in models:\n",
    "            size_mb = model_file.stat().st_size / (1024**2)\n",
    "            total_size += model_file.stat().st_size\n",
    "            model_name = model_file.parent.parent.parent.name.replace('models--', '').replace('--', '/')\n",
    "            print(f\"  üì¶ {model_name}: {size_mb:.0f} MB\")\n",
    "        print(f\"\\nüìä Total cached: {total_size / (1024**3):.2f} GB\")\n",
    "    else:\n",
    "        print(\"\\nüÜï No cached models yet. Will download on first run.\")\n",
    "        print(\"   First run: ~10-15 minutes (includes model downloads)\")\n",
    "        print(\"   - Distil-Whisper: ~1GB (fast transcription)\")\n",
    "        print(\"   - Mistral-7B: ~14GB (fast extraction)\")\n",
    "        print(\"   Next runs: Instant (models cached)\")\n",
    "        print(\"   ‚ö° Optimized for <6s processing per file!\")\n",
    "else:\n",
    "    print(\"\\nüÜï Cache directory created. Ready for downloads!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63a3ba3",
   "metadata": {},
   "source": [
    "## Step 6Ô∏è‚É£: Upload Audio Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40ee5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Create upload directory\n",
    "os.makedirs('uploaded_audio', exist_ok=True)\n",
    "\n",
    "print(\"üìÅ Upload audio files:\")\n",
    "print(\"1. Click 'Choose Files'\")\n",
    "print(\"2. Select multiple audio files (WAV, MP3, FLAC, M4A)\")\n",
    "print(\"3. Wait for upload to complete\\n\")\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "print(f\"\\n‚úÖ {len(uploaded)} files uploaded:\")\n",
    "for filename in uploaded.keys():\n",
    "    file_path = f'uploaded_audio/{filename}'\n",
    "    os.rename(filename, file_path)\n",
    "    file_size = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    print(f\"  üìÑ {filename} ({file_size:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6837016",
   "metadata": {},
   "source": [
    "## Step 7Ô∏è‚É£: Upload Extraction Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ee438",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "# Upload your prompt file\n",
    "print(\"üìù Upload extraction prompt file:\")\n",
    "print(\"Click 'Choose Files' and select your prompt.txt file\\n\")\n",
    "\n",
    "prompt_files = files.upload()\n",
    "\n",
    "if prompt_files:\n",
    "    prompt_filename = list(prompt_files.keys())[0]\n",
    "    os.rename(prompt_filename, 'colab_prompt.txt')\n",
    "    print(f\"‚úÖ Prompt uploaded: {prompt_filename}\")\n",
    "    \n",
    "    # Show first 200 chars\n",
    "    with open('colab_prompt.txt', 'r') as f:\n",
    "        content = f.read()\n",
    "    print(f\"\\nüìñ Prompt preview ({len(content)} chars):\")\n",
    "    print(content[:200] + \"...\\n\" if len(content) > 200 else content)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No prompt file uploaded. Using default prompt.\")\n",
    "    # Create default prompt\n",
    "    default_prompt = \"\"\"Extract stock price information from the transcript.\n",
    "\n",
    "Return JSON with these fields:\n",
    "- index_name: Stock index name (e.g., \\\"S&P 500\\\")\n",
    "- price: Current price\n",
    "- change: Change in points\n",
    "- change_percent: Percent change\n",
    "\n",
    "Return ONLY valid JSON, no explanation.\"\"\"\n",
    "    \n",
    "    with open('colab_prompt.txt', 'w') as f:\n",
    "        f.write(default_prompt)\n",
    "    print(\"‚úÖ Default prompt created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c240f8",
   "metadata": {},
   "source": [
    "## Step 8Ô∏è‚É£: Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c96c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add project to path\n",
    "project_path = Path.cwd()\n",
    "sys.path.insert(0, str(project_path))\n",
    "\n",
    "# Import functions\n",
    "from app.models.transcribe import transcribe\n",
    "from app.models.llm_extract import extract_with_long_prompt\n",
    "\n",
    "print(\"‚úÖ Functions imported successfully!\")\n",
    "print(f\"  üìç transcribe() - ready\")\n",
    "print(f\"  üìç extract_with_long_prompt() - ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e724c481",
   "metadata": {},
   "source": [
    "## Step 9Ô∏è‚É£: Batch Processing (with Timing & Drive Cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298553be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import gc\n",
    "import time\n",
    "\n",
    "# Find all audio files\n",
    "audio_files = sorted(list(Path('uploaded_audio').glob('*')))\n",
    "audio_files = [f for f in audio_files if f.suffix.lower() in ['.wav', '.mp3', '.flac', '.m4a', '.ogg']]\n",
    "\n",
    "print(f\"üìÅ Found {len(audio_files)} audio files\\n\")\n",
    "\n",
    "if len(audio_files) == 0:\n",
    "    print(\"‚ùå No audio files to process\")\n",
    "else:\n",
    "    all_results = {}\n",
    "    batch_start_time = time.time()\n",
    "    \n",
    "    print(\"üîÑ PROCESSING AUDIO FILES (Optimized for <6s per file)\")\n",
    "    print(\"   ‚ö° Using Distil-Whisper + Mistral-7B\")\n",
    "    print(\"   üíæ Models cached in Google Drive\\n\")\n",
    "    \n",
    "    for i, audio_file in enumerate(audio_files, 1):\n",
    "        filename = audio_file.name\n",
    "        file_start_time = time.time()\n",
    "        print(f\"[{i}/{len(audio_files)}] Processing {filename}...\", end=' ', flush=True)\n",
    "        \n",
    "        try:\n",
    "            # Clear memory\n",
    "            torch.cuda.empty_cache()\n",
    "            gc.collect()\n",
    "            \n",
    "            # Transcribe\n",
    "            trans_start = time.time()\n",
    "            result = transcribe(str(audio_file))\n",
    "            trans_duration = time.time() - trans_start\n",
    "            \n",
    "            transcript = result.get('result') if isinstance(result, dict) else result\n",
    "            trans_time = result.get('time', trans_duration) if isinstance(result, dict) else trans_duration\n",
    "            \n",
    "            # Extract\n",
    "            extract_start = time.time()\n",
    "            extraction = extract_with_long_prompt(transcript, prompt_file='colab_prompt.txt')\n",
    "            extract_duration = time.time() - extract_start\n",
    "            \n",
    "            file_total = time.time() - file_start_time\n",
    "            \n",
    "            all_results[filename] = {\n",
    "                \"status\": \"success\",\n",
    "                \"data\": extraction,\n",
    "                \"timing\": {\n",
    "                    \"transcription_s\": round(trans_time, 3),\n",
    "                    \"extraction_s\": round(extract_duration, 3),\n",
    "                    \"total_s\": round(file_total, 3)\n",
    "                }\n",
    "            }\n",
    "            # Check if within target\n",
    "            if file_total < 6.0:\n",
    "                print(f\"‚úÖ ({file_total:.2f}s) ‚ö°\")\n",
    "            else:\n",
    "                print(f\"‚úÖ ({file_total:.2f}s) ‚ö†Ô∏è (target: <6s)\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            file_total = time.time() - file_start_time\n",
    "            all_results[filename] = {\n",
    "                \"status\": \"error\",\n",
    "                \"error\": str(e)[:100],\n",
    "                \"timing\": {\n",
    "                    \"total_s\": round(file_total, 3)\n",
    "                }\n",
    "            }\n",
    "            print(f\"‚ùå ({file_total:.2f}s)\")\n",
    "    \n",
    "    # Save results\n",
    "    with open('batch_results.json', 'w') as f:\n",
    "        json.dump(all_results, f, indent=2)\n",
    "    \n",
    "    batch_total_time = time.time() - batch_start_time\n",
    "    print(f\"\\n‚úÖ All results saved to: batch_results.json\")\n",
    "    \n",
    "    # Summary with timing\n",
    "    success = sum(1 for r in all_results.values() if r[\"status\"] == \"success\")\n",
    "    failed = len(all_results) - success\n",
    "    avg_time = sum(r.get(\"timing\", {}).get(\"total_s\", 0) for r in all_results.values() if r[\"status\"] == \"success\") / max(success, 1)\n",
    "    \n",
    "    print(f\"\\nüìä Summary:\")\n",
    "    print(f\"  ‚úÖ Successful: {success}/{len(all_results)}\")\n",
    "    print(f\"  ‚ùå Failed: {failed}/{len(all_results)}\")\n",
    "    print(f\"  ‚è±Ô∏è Batch Total: {batch_total_time:.2f}s\")\n",
    "    print(f\"  ‚è±Ô∏è Average per file: {avg_time:.2f}s\")\n",
    "    \n",
    "    # Performance check\n",
    "    if avg_time < 6.0:\n",
    "        print(f\"  ‚ö° Performance: EXCELLENT (target: <6s)\")\n",
    "    elif avg_time < 8.0:\n",
    "        print(f\"  ‚ö†Ô∏è Performance: GOOD (target: <6s, actual: {avg_time:.2f}s)\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è Performance: SLOW (target: <6s, actual: {avg_time:.2f}s)\")\n",
    "        print(f\"     üí° Check GPU is enabled and has enough memory\")\n",
    "    \n",
    "    print(f\"\\nüíæ Google Drive Cache Info:\")\n",
    "    cache_dir = Path('/content/drive/MyDrive/Stock-Price-Extractor-Cache')\n",
    "    if cache_dir.exists():\n",
    "        total_cache = sum(f.stat().st_size for f in cache_dir.rglob('*') if f.is_file())\n",
    "        print(f\"  üìÅ Cache Directory: {cache_dir}\")\n",
    "        print(f\"  üìä Total Cached: {total_cache / (1024**3):.2f} GB\")\n",
    "        print(f\"  ‚úÖ Persisted across sessions!\")\n",
    "    \n",
    "    # Show sample results with timing\n",
    "    if success > 0:\n",
    "        print(f\"\\nüìà Sample Results:\")\n",
    "        for filename, result in list(all_results.items())[:3]:\n",
    "            if result[\"status\"] == \"success\":\n",
    "                print(f\"\\n  {filename}:\")\n",
    "                data = result[\"data\"]\n",
    "                timing = result.get(\"timing\", {})\n",
    "                print(f\"    Index: {data.get('index_name')}\")\n",
    "                print(f\"    Price: {data.get('price')}\")\n",
    "                print(f\"    Change: {data.get('change')} ({data.get('change_percent')})\")\n",
    "                print(f\"    üé§ Transcription: {timing.get('transcription_s', 0):.3f}s\")\n",
    "                print(f\"    ü§ñ Extraction: {timing.get('extraction_s', 0):.3f}s\")\n",
    "                print(f\"    ‚è±Ô∏è Total: {timing.get('total_s', 0):.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781e4211",
   "metadata": {},
   "source": [
    "## Step üîü: Download Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b25c70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"üì• Download your results:\\n\")\n",
    "\n",
    "# Download batch results if exists\n",
    "if Path('batch_results.json').exists():\n",
    "    print(\"1. Downloading batch_results.json (with timing data)...\")\n",
    "    files.download('batch_results.json')\n",
    "    print(\"   ‚úÖ Downloaded!\")\n",
    "\n",
    "print(\"\\n‚úÖ All files ready for download!\")\n",
    "print(\"\\nCheck the 'Files' tab (left panel) to download any files.\")\n",
    "print(\"\\nüìä Results include:\")\n",
    "print(\"  - Index name, price, change, change%\")\n",
    "print(\"  - üé§ Transcription time (target: <2s)\")\n",
    "print(\"  - ü§ñ Extraction time (target: <3s)\")\n",
    "print(\"  - ‚è±Ô∏è Total time per file (target: <6s)\")\n",
    "\n",
    "print(\"\\n‚ö° Speed Optimizations:\")\n",
    "print(\"  - Distil-Whisper: Fast transcription (~1-1.5s)\")\n",
    "print(\"  - Mistral-7B: Fast extraction (~2-3s)\")\n",
    "print(\"  - Optimized settings: Zero temperature, reduced tokens\")\n",
    "\n",
    "print(\"\\nüíæ Google Drive Model Cache:\")\n",
    "print(\"  - Models saved in: /MyDrive/Stock-Price-Extractor-Cache\")\n",
    "print(\"  - Reused in next Colab sessions (no re-download!)\")\n",
    "print(\"  - Saves 10-15 minutes on next run! üöÄ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e700785",
   "metadata": {},
   "source": [
    "## üéâ Complete!\n",
    "\n",
    "Your stock price extraction is done! üöÄ\n",
    "\n",
    "**‚ú® What's Special:**\n",
    "1. ‚úÖ Stock prices extracted from audio using **Mistral-7B**\n",
    "2. ‚ö° **Optimized for <6s processing per file**\n",
    "3. üé§ Transcription: <2s (Distil-Whisper)\n",
    "4. ü§ñ LLM extraction: <3s (Mistral-7B)\n",
    "5. ‚è±Ô∏è Total processing time tracked for each file\n",
    "6. üíæ **Models saved to Google Drive (persistent!)**\n",
    "\n",
    "**Performance:**\n",
    "- **Target**: <6 seconds per file\n",
    "- **Transcription**: Distil-Whisper (fast, accurate)\n",
    "- **Extraction**: Mistral-7B (fast, open-source)\n",
    "- **GPU Required**: For <6s target (T4 or better)\n",
    "\n",
    "**Next Session:**\n",
    "- Run this notebook again with new audio files\n",
    "- Models are already in Google Drive\n",
    "- **Skip model downloads (saves 10-15 minutes!)**\n",
    "- Just upload new audio ‚Üí Process ‚Üí Download results\n",
    "\n",
    "**Next steps:**\n",
    "1. Download `batch_results.json` from the Files panel\n",
    "2. Check timing data - should be <6s per file\n",
    "3. Import results into your spreadsheet or database\n",
    "\n",
    "**For more info:**\n",
    "- See `README.md` for general documentation\n",
    "- See `SPEED_OPTIMIZATIONS.md` for speed optimization details\n",
    "- See `USAGE.md` for usage examples"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
